# AzuredataFactory-parquet:
Project 1 revolves around the basic implementation of Extract, Transform, and Load (ETL) operations within Azure Data Factory. The project incorporates data flow, and the primary objective involves handling two distinct Excel files, each containing different tables. Initially, the data from these files will be stored in Azure Blob Storage, a designated resource within the Azure environment. Utilizing Azure Data Factory (ADF), the data extraction process involves fetching data from both Excel files, performing a join operation in the data flow of the factory, and ultimately saving the resultant compressed file in the form of a Parquet file.Parquet files are chosen for their superior compression capabilities, making them more space-efficient compared to formats like Excel and XML when storing information in Blob Storage. The initial steps within Data Factory include creating necessary resources, such as a storage account to house the Excel files. This involves providing a unique name for the storage account in the instance details, followed by a validation check and creation confirmation. Subsequently, within Data Factory, a new instance is created by specifying the instance details and selecting the region. The progression involves the creation of a Blob Storage container folder, where two Excel files are uploaded. Transitioning back to Data Factory, the studio is launched to further proceed with the ETL operations. Pipelines are built and the parquet file is thus stored there.
